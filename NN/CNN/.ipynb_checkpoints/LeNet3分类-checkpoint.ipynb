{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c351a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98267e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cdd88989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02255f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40360013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LeNet, self).__init__(**kwargs)\n",
    "        self.c1 = nn.Conv2d(3, 6, kernel_size=5) # (6, 60, 60)\n",
    "        self.a1 = nn.Sigmoid()\n",
    "        self.p1 = nn.MaxPool2d(kernel_size=2, stride=2) # (6, 30, 30)\n",
    "        self.c2 = nn.Conv2d(6, 16, kernel_size=5) # (16, 26, 26)\n",
    "        self.a2 = nn.Sigmoid()\n",
    "        self.p2 = nn.MaxPool2d(kernel_size=2, stride=2) # (16, 13, 13)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.f1 = nn.Linear(16 * 13 * 13, 240)\n",
    "        self.a3 = nn.Sigmoid()\n",
    "        self.f2 = nn.Linear(240, 84)\n",
    "        self.a4 = nn.Sigmoid()\n",
    "        self.f3 = nn.Linear(84, 3)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.c1(X)\n",
    "        X = self.a1(X)\n",
    "        X = self.p1(X)\n",
    "        X = self.c2(X)\n",
    "        X = self.a2(X)\n",
    "        X = self.p2(X)\n",
    "        print(X.shape)\n",
    "        X = self.flatten(X)\n",
    "        print(X.shape)\n",
    "        X = self.f1(X)\n",
    "        X = self.a3(X)\n",
    "        X = self.f2(X)\n",
    "        X = self.a4(X)\n",
    "        X = self.f3(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c9a2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cec45328",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8a93619",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "max_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb87cd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cfbe3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 169])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    net.to(device)\n",
    "    \n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0968c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "y = flatten(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "330c39b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 169])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5434cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
